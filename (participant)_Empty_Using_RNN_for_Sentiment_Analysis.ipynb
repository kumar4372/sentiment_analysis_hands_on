{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(participant) Empty_Using RNN for Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumar4372/sentiment_analysis_hands_on/blob/master/(participant)_Empty_Using_RNN_for_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DYHYfgAauWJ"
      },
      "source": [
        "# **Sentiment Analysis Using Recurrent Neural Network**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXpXUF4cbI0b"
      },
      "source": [
        "In this tutorial, we will use RNN for sentiment analysis task on movie review dataset.\n",
        "\n",
        "**What is sentiment analysis?**\n",
        "\n",
        "Sentiment Analysis is nothing but finding the sentiments of reviews whether it is positive or negative review.\n",
        "\n",
        "**Example Code to refer**: https://slundberg.github.io/shap/notebooks/deep_explainer/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html\n",
        "\n",
        "**Notes**\n",
        "- RNNs are tricky. Choice of batch size is important,\n",
        "choice of loss and optimizer is critical, etc.\n",
        "Some configurations won't converge.\n",
        "- LSTM loss decrease patterns during training can be quite different\n",
        "from what you see with CNNs/MLPs/etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAZNYKDpbL_4"
      },
      "source": [
        "**Importing Libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzhfuZIPuajX"
      },
      "source": [
        "We start by importing the required dependencies to preprocess our data and build our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jZZT3W6j60O"
      },
      "source": [
        "# Import the dependencies\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN,LSTM, GRU\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "print(\"Imported dependencies.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWcnwxoJjnxd"
      },
      "source": [
        "**Loading Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRzugSCquNN5"
      },
      "source": [
        "We will use IMDB sentiment classification dataset which consists of 50,000 movie reviews from IMDB users that are labeled as either positive (1) or negative (0). \n",
        "\n",
        "Continue downloading the IMDB dataset, which is, fortunately, already built into Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ4aROfwKo9L"
      },
      "source": [
        "# enter your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKwL1Y-aslN7"
      },
      "source": [
        "**Exploring the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3Ya_GxcuETI"
      },
      "source": [
        "You can see in the output above that the dataset is labeled into two categories, — 0 or 1, which represents the sentiment of the review. The whole dataset contains 9,998 unique words and the average review length is 234 words, with a standard deviation of 173 words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fNtIeyN1Z7h"
      },
      "source": [
        "# enter your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLWk5R9huA5l"
      },
      "source": [
        "You can see the first review of the dataset, which is labeled as positive (1). The code below retrieves the dictionary mapping word indices back into the original words so that we can read them. It replaces every unknown word with a “#”. It does this by using the get_word_index() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41v7yfeIZimR"
      },
      "source": [
        "index = imdb.get_word_index()\n",
        "\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
        "\n",
        "print(\" \".join( [reverse_index.get(i - 3, \"#\") for i in x_train[0]] ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtEQG_0nqzz3"
      },
      "source": [
        "**Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B-OZoHct2qi"
      },
      "source": [
        "Now it's time to prepare our data. \n",
        "\n",
        "As we know, each review consists of different number of words. Some reviews could even be one word long. e.g. \"nice\"\n",
        "\n",
        "Deep learning models look best when all of the data is in a similar shape. \n",
        "\n",
        "Here we consider maximum length of our input sequence to be 100. pad_sequences will add 0's to any reviews which don't have a length of 100.\n",
        "\n",
        "For example, our one word review above would become: \"index(nice) 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0... 99 times\"\n",
        "\n",
        "The same goes for any reviews longer than 100 characters, they will be shortened to a maximum of 100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTRzYhyVLG5X"
      },
      "source": [
        "# enter your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfupbfuaqg0f"
      },
      "source": [
        "**BUILDING AND TRAINING THE MODEL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPqDpti_tjHd"
      },
      "source": [
        "Now our data is ready for some modelling!\n",
        "\n",
        "Deep learning models have layers.\n",
        "\n",
        "The top layer takes in the data we've just prepared, the middle layers do some math on this data and the final layer produces an output we can hopefully make use of.\n",
        "\n",
        "In our case, our model has three layers, \n",
        "\n",
        "1. Embedding layer\n",
        "2. LSTM layer\n",
        "3. Dense layer.\n",
        "\n",
        "Our model begins with the line model = Sequential(). Think of this as simply stating \"our model will flow from input to output layer in a sequential manner\" or \"our model goes one step at a time\".\n",
        "\n",
        "**Embedding layer**\n",
        "\n",
        "The Embedding layer creates a database of the relationships between words.\n",
        "\n",
        "model.add(Embedding(max_words, embedding_vector_length, input_length=max_review_length)) is saying: add an Embedding layer to our model and use it to turn each of our words into 32 dimensional vector which have some mathematical relationship to each other.\n",
        "\n",
        "So each of our words will become vectors of dimension 32.\n",
        "\n",
        "For example, vector of \"the\" = [0.556433, 0.223122, 0.789654....].\n",
        "\n",
        "Don't worry for now how this is computed, Keras does it for us.\n",
        "\n",
        "**LSTM layer**\n",
        "\n",
        "model.add(LSTM(128)) is saying: add a LSTM layer after our embedding layer in our model and give it 128 units.\n",
        "\n",
        "LSTM = Long short-term memory. Think of LSTM's as a tap, a tap whichs decides which words flow through the model and which words don't. This layer uses 100 taps to decide which words matter the most in each review.\n",
        "\n",
        "**Dense layer**\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid')) is saying: add a Dense layer to the end of our model and use a sigmoid activation function to produce a meaningful output.\n",
        "\n",
        "A dense layer is also known as a fully-connected layer. This layer connects the 128 LSTM units in the previous layer to 1 unit. This last unit them takes all this information and runs it through a sigmoid function.\n",
        "\n",
        "Essentially, the sigmoid function will decide if the information should be given a 1 or a -1. 1 for positive and -1 for negative. This is will decided on based on the information passed through by the LSTM layer.\n",
        "\n",
        "\n",
        "Lastly, we let Keras print a summary of the model we have just built."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwu39HI97hcm"
      },
      "source": [
        "# enter your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6807KVQhXRz"
      },
      "source": [
        "**Compiling the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-FzmE3mqXYc"
      },
      "source": [
        "Now we compile our model, which is nothing but configuring the model for training. We use the “adam” optimizer, an algorithm that changes the weights and biases during training. We also choose \"binary_crossentropy\" as loss (because we deal with binary classification) and \"accuracy\" as our evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghfPqcms-S2V"
      },
      "source": [
        "# enter your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hzb0SrkhyBM"
      },
      "source": [
        "**Summarize the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYvRQw0uh4yr"
      },
      "source": [
        "Making a summary of the model will give us an idea of what's happening at each layer.\n",
        "\n",
        "In the embedding layer, each of our words is being turned into a vector of dimension 32. Because there are 10000 words (max_words), there are 320,000 parameters (32 x 10000).\n",
        "\n",
        "Parameters are individual pieces of information. The goal of the model is to take a large number of parameters and reduce them down to something we can understand and make use of (less parameters).\n",
        "\n",
        "The LSTM layer reduces the number of parameters to 82432 = 4 × [128(128+32) + 128].\n",
        "\n",
        "The final dense layer connects each of the outputs of the LSTM units into one cell (128 + 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C36PeZgiKjI"
      },
      "source": [
        "# enter your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JKKMT6ZiR9Y"
      },
      "source": [
        "**Fitting the model to the training data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz3QpxeuiW2o"
      },
      "source": [
        "Now our model is compiled, it's ready to be set loose on our training data.\n",
        "\n",
        "We'll be training for 3 epochs with a batch_size of 64.\n",
        "\n",
        "Because of our loss and optimzation functions, the model accuracy should improve after each cycle.\n",
        "\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=64) is saying: fit the model we've built on the training dataset for 3 cycles and go over 64 reviews at a time.\n",
        "\n",
        "Feel free to change the number of epochs (more cycles) or batch_size (more or less information each step) to see how the accuracy changes.\n",
        "\n",
        "This will take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STmd3VIPpAHk"
      },
      "source": [
        "# enter your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn2g3ZRoqGK_"
      },
      "source": [
        "It is time to evaluate our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDRMdH__lapB"
      },
      "source": [
        "# enter your code"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}