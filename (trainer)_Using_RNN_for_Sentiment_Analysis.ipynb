{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(trainer) Using RNN for Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumar4372/sentiment_analysis_hands_on/blob/master/(trainer)_Using_RNN_for_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DYHYfgAauWJ"
      },
      "source": [
        "# **Sentiment Analysis Using Recurrent Neural Network**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PpfRxiBX7HT"
      },
      "source": [
        "## choose Hardware accelarator to \"GPU\" for faster computation. Go to \"Runtime\" -> \"Change runtime type\" to change it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXpXUF4cbI0b"
      },
      "source": [
        "In this tutorial, we will use RNN for sentiment analysis task on movie review dataset.\n",
        "\n",
        "**What is sentiment analysis?**\n",
        "\n",
        "Sentiment Analysis is nothing but finding the sentiments of reviews whether it is positive or negative review.\n",
        "\n",
        "**Example Code to refer**: https://slundberg.github.io/shap/notebooks/deep_explainer/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html\n",
        "\n",
        "**Notes**\n",
        "- RNNs are tricky. Choice of batch size is important,\n",
        "choice of loss and optimizer is critical, etc.\n",
        "Some configurations won't converge.\n",
        "- LSTM loss decrease patterns during training can be quite different\n",
        "from what you see with CNNs/MLPs/etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAZNYKDpbL_4"
      },
      "source": [
        "**Importing Libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzhfuZIPuajX"
      },
      "source": [
        "We start by importing the required dependencies to preprocess our data and build our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jZZT3W6j60O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86b46f6a-97c3-4afb-f279-a7f212e8257a"
      },
      "source": [
        "# Import the dependencies\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Dense, SimpleRNN,LSTM, GRU\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "print(\"Imported dependencies.\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imported dependencies.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWcnwxoJjnxd"
      },
      "source": [
        "**Loading Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRzugSCquNN5"
      },
      "source": [
        "We will use IMDB sentiment classification dataset which consists of 50,000 movie reviews from IMDB users that are labeled as either positive (1) or negative (0). \n",
        "\n",
        "Continue downloading the IMDB dataset, which is, fortunately, already built into Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlgiU4PSkLUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a9b5428-174f-4959-ef27-fe1bd6a11ecc"
      },
      "source": [
        "vocab_size = 10000\n",
        "\n",
        "# Define the training and test dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)  # vocab_size is no. of words to consider from the dataset, ordering based on frequency.\n",
        "\n",
        "print(\"Created test and training data.\")\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/keras/datasets/imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Created test and training data.\n",
            "25000 train sequences\n",
            "25000 test sequences\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/datasets/imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKwL1Y-aslN7"
      },
      "source": [
        "**Exploring the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmS3ls6YyVLN"
      },
      "source": [
        "You can see in the output above that the dataset is labeled into two categories, — 0 or 1, which represents the sentiment of the review. The whole dataset contains 9,998 unique words and the average review length is 234 words, with a standard deviation of 173 words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RODKEpHUlaH8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c996a269-07e5-4440-a765-f4e8b14903a5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#concatenate whole data\n",
        "data = np.concatenate((x_train, x_test), axis=0)\n",
        "targets = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "print(\"Categories:\", np.unique(targets))\n",
        "print(\"Number of unique words:\", len(np.unique(np.hstack(data))))\n",
        "length = [len(i) for i in data]\n",
        "print(\"Average Review length:\", np.mean(length))\n",
        "print(\"Standard Deviation:\", round(np.std(length)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Categories: [0 1]\n",
            "Number of unique words: 9998\n",
            "Average Review length: 234.75892\n",
            "Standard Deviation: 173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3Ya_GxcuETI"
      },
      "source": [
        "You can see the first review of the dataset, which is labeled as positive (1). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdOq8BHEmEXg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3f4ec78-44c5-4355-a41a-a0a847d5dc53"
      },
      "source": [
        "print('---review---')\n",
        "print(x_train[0])\n",
        "print('---label---')\n",
        "print(y_train[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---review---\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "---label---\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLWk5R9huA5l"
      },
      "source": [
        "Above you can see the first review of the dataset, which is labeled as positive (1). The code below retrieves the dictionary mapping word indices back into the original words so that we can read them. It replaces every unknown word with a “#”. It does this by using the get_word_index() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TW5kQUWmQDP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff5cf24c-0f21-4032-fa4a-000f8614c276"
      },
      "source": [
        "index = imdb.get_word_index()\n",
        "train_text = []\n",
        "test_text = []\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
        "for i in range(0,len(x_train)):\n",
        "  train_text.append(\" \".join( [reverse_index.get(i - 3, \"#\") for i in x_train[i]] ))\n",
        "for i in range(0,len(x_test)):\n",
        "  test_text.append(\" \".join( [reverse_index.get(i - 3, \"#\") for i in x_train[i]] ))\n",
        "print(len(train_text),len(test_text))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adNJ_dvIV566",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28131618-3b81-4780-fda8-dec50d10e47e"
      },
      "source": [
        "print(train_text[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for # and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also # to the two little boy's that played the # of norman and paul they were just brilliant children are often left out of the # list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtEQG_0nqzz3"
      },
      "source": [
        "**Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B-OZoHct2qi"
      },
      "source": [
        "Now it's time to prepare our data. \n",
        "\n",
        "As we know, each review consists of different number of words. Some reviews could even be one word long. e.g. \"nice\"\n",
        "\n",
        "Deep learning models look best when all of the data is in a similar shape. \n",
        "\n",
        "Here we consider maximum length of our input sequence to be 100. pad_sequences will add 0's to any reviews which don't have a length of 100.\n",
        "\n",
        "For example, our one word review above would become: \"index(nice) 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0... 99 times\"\n",
        "\n",
        "The same goes for any reviews longer than 100 characters, they will be shortened to a maximum of 100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNS1Jr-omVC3"
      },
      "source": [
        "max_review_length = 100\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_review_length)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_eUysG_fucz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "079a587c-a58c-47e7-8ad2-11a5d507a8de"
      },
      "source": [
        "print(x_train[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1415   33    6   22   12  215   28   77   52    5   14  407   16   82\n",
            "    2    8    4  107  117 5952   15  256    4    2    7 3766    5  723\n",
            "   36   71   43  530  476   26  400  317   46    7    4    2 1029   13\n",
            "  104   88    4  381   15  297   98   32 2071   56   26  141    6  194\n",
            " 7486   18    4  226   22   21  134  476   26  480    5  144   30 5535\n",
            "   18   51   36   28  224   92   25  104    4  226   65   16   38 1334\n",
            "   88   12   16  283    5   16 4472  113  103   32   15   16 5345   19\n",
            "  178   32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfupbfuaqg0f"
      },
      "source": [
        "**BUILDING AND TRAINING THE MODEL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPqDpti_tjHd"
      },
      "source": [
        "Now our data is ready for some modelling!\n",
        "\n",
        "Deep learning models have layers.\n",
        "\n",
        "The top layer takes in the data we've just prepared, the middle layers do some math on this data and the final layer produces an output we can hopefully make use of.\n",
        "\n",
        "In our case, our model has three layers, \n",
        "\n",
        "1. Embedding layer\n",
        "2. LSTM layer\n",
        "3. Dense layer.\n",
        "\n",
        "Our model begins with the line model = Sequential(). Think of this as simply stating \"our model will flow from input to output layer in a sequential manner\" or \"our model goes one step at a time\".\n",
        "\n",
        "**Embedding layer**\n",
        "\n",
        "The Embedding layer creates a database of the relationships between words.\n",
        "\n",
        "model.add(Embedding(max_words, embedding_vector_length, input_length=max_review_length)) is saying: add an Embedding layer to our model and use it to turn each of our words into 32 dimensional vector which have some mathematical relationship to each other.\n",
        "\n",
        "So each of our words will become vectors of dimension 32.\n",
        "\n",
        "For example, vector of \"the\" = [0.556433, 0.223122, 0.789654....].\n",
        "\n",
        "Don't worry for now how this is computed, Keras does it for us.\n",
        "\n",
        "**LSTM layer**\n",
        "\n",
        "model.add(LSTM(128)) is saying: add a LSTM layer after our embedding layer in our model and give it 128 units.\n",
        "\n",
        "LSTM = Long short-term memory. Think of LSTM's as a tap, a tap whichs decides which words flow through the model and which words don't. This layer uses 100 taps to decide which words matter the most in each review.\n",
        "\n",
        "**Dense layer**\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid')) is saying: add a Dense layer to the end of our model and use a sigmoid activation function to produce a meaningful output.\n",
        "\n",
        "A dense layer is also known as a fully-connected layer. This layer connects the 128 LSTM units in the previous layer to 1 unit. This last unit them takes all this information and runs it through a sigmoid function.\n",
        "\n",
        "Essentially, the sigmoid function will decide if the information should be given a 1 or a -1. 1 for positive and -1 for negative. This is will decided on based on the information passed through by the LSTM layer.\n",
        "\n",
        "\n",
        "Lastly, we let Keras print a summary of the model we have just built."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwu39HI97hcm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70fc6706-b484-4efb-bdb7-7b923af3f543"
      },
      "source": [
        "# Define how long the embedding vector will be\n",
        "embedding_vector_length = 128\n",
        "\n",
        "# Define the layers in the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_vector_length, input_length=vocab_size))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "print(\"Model created.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6807KVQhXRz"
      },
      "source": [
        "**Compiling the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-FzmE3mqXYc"
      },
      "source": [
        "Now we compile our model, which is nothing but configuring the model for training. We use the “adam” optimizer, an algorithm that changes the weights and biases during training. We also choose binary-crossentropy as loss (because we deal with binary classification) and accuracy as our evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghfPqcms-S2V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21db5513-96a3-4e17-e943-e9cb5a5ec301"
      },
      "source": [
        "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(\"Model compiled, ready to be fit to the training data.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model compiled, ready to be fit to the training data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hzb0SrkhyBM"
      },
      "source": [
        "**Summarize the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYvRQw0uh4yr"
      },
      "source": [
        "Making a summary of the model will give us an idea of what's happening at each layer.\n",
        "\n",
        "In the embedding layer, each of our words is being turned into a vector of dimension 128. Because there are 10000 words (max_words), there are 1,280,000 parameters (128 x 10000).\n",
        "\n",
        "Parameters are individual pieces of information. The goal of the model is to take a large number of parameters and reduce them down to something we can understand and make use of (less parameters).\n",
        "\n",
        "The LSTM layer reduces the number of parameters to 82432 = 4 × [128(128+32) + 128].\n",
        "\n",
        "The final dense layer connects each of the outputs of the LSTM units into one cell (128 + 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C36PeZgiKjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4df96286-9ba7-4e16-c74e-4f37b558cb9e"
      },
      "source": [
        "# Summarize the different layers in the model\n",
        "print(model.summary())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 10000, 128)        1280000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 1,411,713\n",
            "Trainable params: 1,411,713\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JKKMT6ZiR9Y"
      },
      "source": [
        "**Fitting the model to the training data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz3QpxeuiW2o"
      },
      "source": [
        "Now our model is compiled, it's ready to be set loose on our training data.\n",
        "\n",
        "We'll be training for 3 epochs with a batch_size of 64.\n",
        "\n",
        "Because of our loss and optimzation functions, the model accuracy should improve after each cycle.\n",
        "\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=64) is saying: fit the model we've built on the training dataset for 3 cycles and go over 64 reviews at a time.\n",
        "\n",
        "Feel free to change the number of epochs (more cycles) or batch_size (more or less information each step) to see how the accuracy changes.\n",
        "\n",
        "This will take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STmd3VIPpAHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcfe8799-52b2-40fe-c796-d5119b05a3e9"
      },
      "source": [
        "# Fit the model to the training data\n",
        "results = model.fit(x_train, y_train, epochs=3, batch_size=64,validation_data=(x_test, y_test))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 10000) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10000), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 100).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 10000) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10000), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 100).\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.5462 - accuracy: 0.6988WARNING:tensorflow:Model was constructed with shape (None, 10000) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10000), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 100).\n",
            "391/391 [==============================] - 28s 29ms/step - loss: 0.5454 - accuracy: 0.6995 - val_loss: 0.3688 - val_accuracy: 0.8394\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 0.2623 - accuracy: 0.8949 - val_loss: 0.3545 - val_accuracy: 0.8447\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 12s 29ms/step - loss: 0.1846 - accuracy: 0.9326 - val_loss: 0.3901 - val_accuracy: 0.8388\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn2g3ZRoqGK_"
      },
      "source": [
        "It is time to evaluate our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bs_I_gWbIIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af20649e-1cc6-4092-e9af-fb7bcc4b1824"
      },
      "source": [
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=64)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 2s 5ms/step - loss: 0.3901 - accuracy: 0.8388\n",
            "Test score: 0.39011549949645996\n",
            "Test accuracy: 0.8388000130653381\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXZaFmY5dauk"
      },
      "source": [
        "Let's analyze the results now and look at some examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aETOH47QY_tE",
        "outputId": "f892e4f3-3d09-46d8-9923-53154c12d725"
      },
      "source": [
        "y_pred = model.predict_classes(x_test)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 10000) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10000), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 100).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrvqtcqVZXsp",
        "outputId": "1ecb8f5c-e8da-495b-fb6a-736c51d4191d"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       ...,\n",
              "       [0],\n",
              "       [0],\n",
              "       [0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awHoVtqOZ5gj",
        "outputId": "d496cee5-b5bb-4566-ca29-e6b39445887d"
      },
      "source": [
        "y_pred.reshape([len(y_pred)])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, ..., 0, 0, 0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeiAaANaZjUX"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None) # for better printing"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC0uqVH7ZmLx"
      },
      "source": [
        "results = pd.DataFrame({\"review\":test_text, \"ground_truth\":y_test, \"prediction\":y_pred.reshape([len(y_pred)])})"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "id": "Se8vLfYQaBdS",
        "outputId": "81b7910c-d24c-48fa-f4f3-1d7b5a0487b8"
      },
      "source": [
        "results.head(5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td># this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for # and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also # to the two little boy's that played the # of norman and paul they were just brilliant children are often left out of the # list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td># big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal # the hair is big lots of boobs # men wear those cut # shirts that show off their # sickening that men actually wore them and the music is just # trash that plays over and over again in almost every scene there is trashy music boobs and # taking away bodies and the gym still doesn't close for # all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td># this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had # working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how # this is to watch save yourself an hour a bit of your life</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td># the # # at storytelling the traditional sort many years after the event i can still see in my # eye an elderly lady my friend's mother retelling the battle of # she makes the characters come alive her passion is that of an eye witness one to the events on the # heath a mile or so from where she lives br br of course it happened many years before she was born but you wouldn't guess from the way she tells it the same story is told in bars the length and # of scotland as i discussed it with a friend one night in # a local cut in to give his version the discussion continued to closing time br br stories passed down like this become part of our being who doesn't remember the stories our parents told us when we were children they become our invisible world and as we grow older they maybe still serve as inspiration or as an emotional # fact and fiction blend with # role models warning stories # magic and mystery br br my name is # like my grandfather and his grandfather before him our protagonist introduces himself to us and also introduces the story that stretches back through generations it produces stories within stories stories that evoke the # wonder of scotland its rugged mountains # in # the stuff of legend yet # is # in reality this is what gives it its special charm it has a rough beauty and authenticity # with some of the finest # singing you will ever hear br br # # visits his grandfather in hospital shortly before his death he burns with frustration part of him # to be in the twenty first century to hang out in # but he is raised on the western # among a # speaking community br br yet there is a deeper conflict within him he # to know the truth the truth behind his # ancient stories where does fiction end and he wants to know the truth behind the death of his parents br br he is pulled to make a last # journey to the # of one of # most # mountains can the truth be told or is it all in stories br br in this story about stories we # bloody battles # lovers the # of old and the sometimes more # # of accepted truth in doing so we each connect with # as he lives the story of his own life br br # the # # is probably the most honest # and genuinely beautiful film of scotland ever made like # i got slightly annoyed with the # of hanging stories on more stories but also like # i # this once i saw the # picture ' forget the box office # of braveheart and its like you might even # the # famous # of the wicker man to see a film that is true to scotland this one is probably unique if you maybe # on it deeply enough you might even re # the power of storytelling and the age old question of whether there are some truths that cannot be told but only experienced</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td># worst mistake of my life br br i picked this movie up at target for 5 because i figured hey it's sandler i can get some cheap laughs i was wrong completely wrong mid way through the film all three of my friends were asleep and i was still suffering worst plot worst script worst movie i have ever seen i wanted to hit my head up against a wall for an hour then i'd stop and you know why because it felt damn good upon bashing my head in i stuck that damn movie in the # and watched it burn and that felt better than anything else i've ever done it took american psycho army of darkness and kill bill just to get over that crap i hate you sandler for actually going through with this and ruining a whole day of my life</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        review  ...  prediction\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    # this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for # and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also # to the two little boy's that played the # of norman and paul they were just brilliant children are often left out of the # list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all  ...           0\n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                # big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal # the hair is big lots of boobs # men wear those cut # shirts that show off their # sickening that men actually wore them and the music is just # trash that plays over and over again in almost every scene there is trashy music boobs and # taking away bodies and the gym still doesn't close for # all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then  ...           1\n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               # this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had # working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how # this is to watch save yourself an hour a bit of your life  ...           1\n",
              "3  # the # # at storytelling the traditional sort many years after the event i can still see in my # eye an elderly lady my friend's mother retelling the battle of # she makes the characters come alive her passion is that of an eye witness one to the events on the # heath a mile or so from where she lives br br of course it happened many years before she was born but you wouldn't guess from the way she tells it the same story is told in bars the length and # of scotland as i discussed it with a friend one night in # a local cut in to give his version the discussion continued to closing time br br stories passed down like this become part of our being who doesn't remember the stories our parents told us when we were children they become our invisible world and as we grow older they maybe still serve as inspiration or as an emotional # fact and fiction blend with # role models warning stories # magic and mystery br br my name is # like my grandfather and his grandfather before him our protagonist introduces himself to us and also introduces the story that stretches back through generations it produces stories within stories stories that evoke the # wonder of scotland its rugged mountains # in # the stuff of legend yet # is # in reality this is what gives it its special charm it has a rough beauty and authenticity # with some of the finest # singing you will ever hear br br # # visits his grandfather in hospital shortly before his death he burns with frustration part of him # to be in the twenty first century to hang out in # but he is raised on the western # among a # speaking community br br yet there is a deeper conflict within him he # to know the truth the truth behind his # ancient stories where does fiction end and he wants to know the truth behind the death of his parents br br he is pulled to make a last # journey to the # of one of # most # mountains can the truth be told or is it all in stories br br in this story about stories we # bloody battles # lovers the # of old and the sometimes more # # of accepted truth in doing so we each connect with # as he lives the story of his own life br br # the # # is probably the most honest # and genuinely beautiful film of scotland ever made like # i got slightly annoyed with the # of hanging stories on more stories but also like # i # this once i saw the # picture ' forget the box office # of braveheart and its like you might even # the # famous # of the wicker man to see a film that is true to scotland this one is probably unique if you maybe # on it deeply enough you might even re # the power of storytelling and the age old question of whether there are some truths that cannot be told but only experienced  ...           0\n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              # worst mistake of my life br br i picked this movie up at target for 5 because i figured hey it's sandler i can get some cheap laughs i was wrong completely wrong mid way through the film all three of my friends were asleep and i was still suffering worst plot worst script worst movie i have ever seen i wanted to hit my head up against a wall for an hour then i'd stop and you know why because it felt damn good upon bashing my head in i stuck that damn movie in the # and watched it burn and that felt better than anything else i've ever done it took american psycho army of darkness and kill bill just to get over that crap i hate you sandler for actually going through with this and ruining a whole day of my life  ...           1\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQsFZZDCad2T"
      },
      "source": [
        "## some samples of wrong predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "G1tBg9mkaFp7",
        "outputId": "262d7894-a761-41d6-cce9-189ff5b4ac07"
      },
      "source": [
        "results[results['ground_truth'] != results['prediction']].sample(5)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1627</th>\n",
              "      <td># i'm trying to decide if jumping into a wood # would be more enjoyable than this dreck it finishes the destruction of what was once a classic couple of films with jedi menace # and # we have the death of # career he wants us to swallow the # is vader nonsense i never believed it was true this film # those feelings the story hasn't worked since phantom moron and each new film just # the crap on until all that was left was a toy parade i have to go i know where some new rocks to throw are you want spoilers here they come luke and leia are not related vader is not their father duke # should have switched sides while he still could # has less verbal skills than # # his advice has never been any good to anybody # wan # to luke for the first two films # didn't build # he found him in the desert and # to his mom about putting him together from scratch # has # this whole mess with vader and the fall of the republic can be blamed on that stupid b h # # # # if she had any brains she wouldn't have come within a light year of annie but she had told do what george lucas wrote for her what a #</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6549</th>\n",
              "      <td># dark wolf quick review let's get right to it this is a # piece of rotting # with cow sh t on it it's just an awful movie it's an urban werewolf movie with some of the worst acting imaginable and a story as weak as any # nerd from an 80's high school drama film what's worse is that poor kane # was # into playing the gigantic evil werewolf kane f # # someone's trying to ensure that playing jason # is the height of his film career br br anyway former # # bergman is also in the movie and she eventually becomes a werewolf too it's kind of a crappy cop drama with the world's worst looking werewolf in it but it does have moments of near rampant nudity but that's about all want to know more okay the werewolf is generally an ugly looking black # # around the screen and when we're # enough to actually see a transformation sequence we're presented with something that resembles a full motion video from a video game made during the early stages of the # the first # the cg animation is really that primitive only good for horror hardcore fanatics that want to see small moments of nudity surrounded by rampant visual vomit 2 10 br br www # com</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8330</th>\n",
              "      <td># well well well as good as john carpenter's season 1 outing in masters of horror was this is the complete opposite he certainly proved he was still a master of horror with cigarette burns but pro life is perhaps the worst i have seen from him br br it's stupid totally devoid of creepy atmosphere and tension and it # it's welcome despite the less than an hour running time the script is nonsense the characters are # and un appealing and the conclusion is beyond absurd br br and for those # who actually bought the dvd one of them being me did you see how carpenter describes the film he's actually proud of it and he talks about it as his best work for a long time and he # the script and in the commentary track where he notices an obvious screw up that made it to the final cut he just says he didn't feel it essential to # the mistake and he just let it be there i fear the old master has completely lost his touch i sincerely hope i'm proved wrong br br i want to leave on a positive note and mention that the creature effects are awesome though technically speaking this film is top notch with effective lighting schemes and make up effects</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13972</th>\n",
              "      <td># our # of the perverse did good his first time out thats for sure the music is the best you may ever hear by any human but you already know that unless you have no taste or have a brain that is too small to understand greatness a poor script that doesn't flesh out much of a story but at least it has its moments the breathtaking concert stuff is worth seeing it he deserved an oscar for this s t even though he was at times an ego driven # with his # bodyguard chick # always in front a movie that made non fans fans take it or leave it prince does need to stay clear of acting in the future though he takes himself way to serious he is a genius musician but # just enjoy the ride my purple # peace</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10011</th>\n",
              "      <td># please help the economy spend your money elsewhere the synopsis of the movie is the first lady has her husband # because he was cheating on her that's it # by anyone except cuba and angie she designs and # a vast assassination conspiracy which no one knows about and gets away completely free br br some specific points are particularly hilarious while standing in front of the president cuba a # the # bullet which then enters the back of the # head br br cuba and angie watch film from a news camera and they see a clue they go to great # to protect the film believing that they are the only people that have a copy of this very public film br br cuba speaks with a presidential staff member the # comments that there was no conspiracy cuba claims there was more than one person involved the # then # that the conspiracy includes the fbi the cia and the # gosh i wonder is the # is involved br br ms archer the first lady is a # artist cuba can't make out a painting and she says you're too close stand back look from a different perspective look from my perspective can anyone miss that clue</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              review  ...  prediction\n",
              "1627                                                       # i'm trying to decide if jumping into a wood # would be more enjoyable than this dreck it finishes the destruction of what was once a classic couple of films with jedi menace # and # we have the death of # career he wants us to swallow the # is vader nonsense i never believed it was true this film # those feelings the story hasn't worked since phantom moron and each new film just # the crap on until all that was left was a toy parade i have to go i know where some new rocks to throw are you want spoilers here they come luke and leia are not related vader is not their father duke # should have switched sides while he still could # has less verbal skills than # # his advice has never been any good to anybody # wan # to luke for the first two films # didn't build # he found him in the desert and # to his mom about putting him together from scratch # has # this whole mess with vader and the fall of the republic can be blamed on that stupid b h # # # # if she had any brains she wouldn't have come within a light year of annie but she had told do what george lucas wrote for her what a #  ...           0\n",
              "6549      # dark wolf quick review let's get right to it this is a # piece of rotting # with cow sh t on it it's just an awful movie it's an urban werewolf movie with some of the worst acting imaginable and a story as weak as any # nerd from an 80's high school drama film what's worse is that poor kane # was # into playing the gigantic evil werewolf kane f # # someone's trying to ensure that playing jason # is the height of his film career br br anyway former # # bergman is also in the movie and she eventually becomes a werewolf too it's kind of a crappy cop drama with the world's worst looking werewolf in it but it does have moments of near rampant nudity but that's about all want to know more okay the werewolf is generally an ugly looking black # # around the screen and when we're # enough to actually see a transformation sequence we're presented with something that resembles a full motion video from a video game made during the early stages of the # the first # the cg animation is really that primitive only good for horror hardcore fanatics that want to see small moments of nudity surrounded by rampant visual vomit 2 10 br br www # com  ...           0\n",
              "8330   # well well well as good as john carpenter's season 1 outing in masters of horror was this is the complete opposite he certainly proved he was still a master of horror with cigarette burns but pro life is perhaps the worst i have seen from him br br it's stupid totally devoid of creepy atmosphere and tension and it # it's welcome despite the less than an hour running time the script is nonsense the characters are # and un appealing and the conclusion is beyond absurd br br and for those # who actually bought the dvd one of them being me did you see how carpenter describes the film he's actually proud of it and he talks about it as his best work for a long time and he # the script and in the commentary track where he notices an obvious screw up that made it to the final cut he just says he didn't feel it essential to # the mistake and he just let it be there i fear the old master has completely lost his touch i sincerely hope i'm proved wrong br br i want to leave on a positive note and mention that the creature effects are awesome though technically speaking this film is top notch with effective lighting schemes and make up effects  ...           0\n",
              "13972                                                                                                                                                                                                                                                                                                                                                                                                                                                                   # our # of the perverse did good his first time out thats for sure the music is the best you may ever hear by any human but you already know that unless you have no taste or have a brain that is too small to understand greatness a poor script that doesn't flesh out much of a story but at least it has its moments the breathtaking concert stuff is worth seeing it he deserved an oscar for this s t even though he was at times an ego driven # with his # bodyguard chick # always in front a movie that made non fans fans take it or leave it prince does need to stay clear of acting in the future though he takes himself way to serious he is a genius musician but # just enjoy the ride my purple # peace  ...           1\n",
              "10011                                                       # please help the economy spend your money elsewhere the synopsis of the movie is the first lady has her husband # because he was cheating on her that's it # by anyone except cuba and angie she designs and # a vast assassination conspiracy which no one knows about and gets away completely free br br some specific points are particularly hilarious while standing in front of the president cuba a # the # bullet which then enters the back of the # head br br cuba and angie watch film from a news camera and they see a clue they go to great # to protect the film believing that they are the only people that have a copy of this very public film br br cuba speaks with a presidential staff member the # comments that there was no conspiracy cuba claims there was more than one person involved the # then # that the conspiracy includes the fbi the cia and the # gosh i wonder is the # is involved br br ms archer the first lady is a # artist cuba can't make out a painting and she says you're too close stand back look from a different perspective look from my perspective can anyone miss that clue  ...           0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGoFsTbhcdTI"
      },
      "source": [
        "## some samples of correct predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "ihLVvshlaFs2",
        "outputId": "004ff3bc-0201-4f2c-dd0b-ba6b1d849acd"
      },
      "source": [
        "results[results['ground_truth'] == results['prediction']].sample(5)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>24026</th>\n",
              "      <td># having just wasted a couple of hours watching this and for 80 of that time in complete disbelief i can give this garbage the turkey of the year award no problem to say the plot was unbelievable is some big understatement frankly i am lost for words to describe this utter tripe not only are the characters completely and utterly without any semblance of originality this sort of stuff has been done much better in dozens of # killer # but the acting was dire for those who pay to see this i hope you get your money back for those who were paid to do this i hope you give your money back believe me folks there are many new releases out there that are much much better go see</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23694</th>\n",
              "      <td># there is a reason this went straight to video the story is smarmy nick cage plays johnny in a sleazy way sex in # and other scenes that border on tasteless like the scene in the # room # this movie judge # as the # is okay but the movie itself with its themes of # and revenge are not well done but it is a good film for trivia # because so few people saw it</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>761</th>\n",
              "      <td># this movie introduces quite an array of characters and their relationships in the first half hour or so none of them generate any interest or positive response i waited for the intrigue to begin hoping things would get better and ended up sticking around until the bitter end but there was no reward for doing that br br if you want a synopsis look elsewhere to me the action isn't worth # not that the story was that bad i guess you could say i had some problems with the script i e i thought it # a look at the credits will show you that there's a pretty strong cast here used to no # most of the old pros in this flick do good jobs of the actors i hadn't seen much of before i especially liked deborah # # that's about all that i can find good to say about this picture</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5250</th>\n",
              "      <td># if you like plot turns this is your movie it is impossible at any moment to predict what will happen next nothing is as it appears or ends as you think it will the characters are all gritty and engaging cage is at his best dennis hopper again shows his delightfully sinister side # walsh is perfect in his last performance laura boyle # dwight # makes a film debut superbly in a cameo i # this movie as i am having a really really really bad day film not a slow minute in this film a real sleeper this movie is underrated and sadly overlooked</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17121</th>\n",
              "      <td># may anne reid and # peter # are paying an apparently # visit to their son bobby steven # and his family in london even as the visit begins # suffers a fatal heart attack leaving may # unsure and questioning her life and future finding herself attracted to her daughter's boyfriend darren daniel craig her actions lead to inevitable consequences br br beautifully filmed but for all its # realism and acclaim the mother offers a collection of mostly unpleasant even # characters and asks the viewer to engage with them reid shines as may and it is her skill and commitment as a wonderfully understated actor that # the film from a completely depressing # but # and # have allowed craig # and # # to create such utterly obnoxious characters that it becomes increasingly difficult to care what happens to may as written the characters played by # and # are in fact so utterly selfish and cold hearted that one begins to wonder what exactly was # trying to say as directed they are either unwilling or unable to lift bobby and paula above the two dimensional in their ghastly # br br worth seeing for # performance but little else a crying shame</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        review  ...  prediction\n",
              "24026                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     # having just wasted a couple of hours watching this and for 80 of that time in complete disbelief i can give this garbage the turkey of the year award no problem to say the plot was unbelievable is some big understatement frankly i am lost for words to describe this utter tripe not only are the characters completely and utterly without any semblance of originality this sort of stuff has been done much better in dozens of # killer # but the acting was dire for those who pay to see this i hope you get your money back for those who were paid to do this i hope you give your money back believe me folks there are many new releases out there that are much much better go see  ...           1\n",
              "23694                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 # there is a reason this went straight to video the story is smarmy nick cage plays johnny in a sleazy way sex in # and other scenes that border on tasteless like the scene in the # room # this movie judge # as the # is okay but the movie itself with its themes of # and revenge are not well done but it is a good film for trivia # because so few people saw it  ...           0\n",
              "761                                                                                                                                                                                                                                                                                                                                                                                     # this movie introduces quite an array of characters and their relationships in the first half hour or so none of them generate any interest or positive response i waited for the intrigue to begin hoping things would get better and ended up sticking around until the bitter end but there was no reward for doing that br br if you want a synopsis look elsewhere to me the action isn't worth # not that the story was that bad i guess you could say i had some problems with the script i e i thought it # a look at the credits will show you that there's a pretty strong cast here used to no # most of the old pros in this flick do good jobs of the actors i hadn't seen much of before i especially liked deborah # # that's about all that i can find good to say about this picture  ...           0\n",
              "5250                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          # if you like plot turns this is your movie it is impossible at any moment to predict what will happen next nothing is as it appears or ends as you think it will the characters are all gritty and engaging cage is at his best dennis hopper again shows his delightfully sinister side # walsh is perfect in his last performance laura boyle # dwight # makes a film debut superbly in a cameo i # this movie as i am having a really really really bad day film not a slow minute in this film a real sleeper this movie is underrated and sadly overlooked  ...           1\n",
              "17121  # may anne reid and # peter # are paying an apparently # visit to their son bobby steven # and his family in london even as the visit begins # suffers a fatal heart attack leaving may # unsure and questioning her life and future finding herself attracted to her daughter's boyfriend darren daniel craig her actions lead to inevitable consequences br br beautifully filmed but for all its # realism and acclaim the mother offers a collection of mostly unpleasant even # characters and asks the viewer to engage with them reid shines as may and it is her skill and commitment as a wonderfully understated actor that # the film from a completely depressing # but # and # have allowed craig # and # # to create such utterly obnoxious characters that it becomes increasingly difficult to care what happens to may as written the characters played by # and # are in fact so utterly selfish and cold hearted that one begins to wonder what exactly was # trying to say as directed they are either unwilling or unable to lift bobby and paula above the two dimensional in their ghastly # br br worth seeing for # performance but little else a crying shame  ...           0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRpQSZ51f3gp"
      },
      "source": [
        "## Below we have some more sample architectures you can try !!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgPdDWvfVWW_"
      },
      "source": [
        "**Extensions**\n",
        "\n",
        "Let us use LSTM variants. We use check the accuracy by replacing LSTM cell with GRU cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNe2GAad7_rr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27e3be4c-0fa8-4bfa-ccea-bd7344dbab6a"
      },
      "source": [
        "# Define the layers in the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_vector_length, input_length=max_review_length))\n",
        "#model.add(LSTM(128))\n",
        "model.add(GRU(32,dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(model.summary())\n",
        "\n",
        "#num_params_layer 3 × [h(h+i) + h]  = 3 × [32(32+64) + 32] = 9312"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 100, 128)          1280000   \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 32)                15552     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,295,585\n",
            "Trainable params: 1,295,585\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "316Ha7ug8EEu"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "             optimizer='adam', \n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMbarkxdWFHM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcf9f80f-8c15-41b5-a2cf-89642fb7148d"
      },
      "source": [
        "# Fit the model to the training data\n",
        "results = model.fit(x_train, y_train, epochs=3, batch_size=64,validation_data=(x_test, y_test))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "391/391 [==============================] - 145s 362ms/step - loss: 0.5543 - accuracy: 0.6922 - val_loss: 0.3541 - val_accuracy: 0.8450\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 140s 357ms/step - loss: 0.2739 - accuracy: 0.8919 - val_loss: 0.3519 - val_accuracy: 0.8496\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 140s 358ms/step - loss: 0.1959 - accuracy: 0.9269 - val_loss: 0.3628 - val_accuracy: 0.8500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja11LMYJ8cPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c152df42-c099-4ea6-ea4d-79c46823d430"
      },
      "source": [
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=64)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 10s 26ms/step - loss: 0.3628 - accuracy: 0.8500\n",
            "Test score: 0.36276453733444214\n",
            "Test accuracy: 0.8499600291252136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzzYeeiAWPXh"
      },
      "source": [
        "**Using LSTM stack layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIOwL8VH8o_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30dde1d3-ec76-4c15-c8c1-fb63ec64bacb"
      },
      "source": [
        "model= Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_vector_length, input_length=max_review_length))\n",
        "model.add(LSTM(units=16, return_sequences=True))\n",
        "model.add(LSTM(units=8, return_sequences=True))\n",
        "model.add(LSTM(units=4))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(model.summary())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 100, 128)          1280000   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100, 16)           9280      \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100, 8)            800       \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 4)                 208       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 1,290,293\n",
            "Trainable params: 1,290,293\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtMPv6pR8sBW"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "             optimizer='adam', \n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0_eb8nT8s0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d85231-a3d1-4d4e-b101-b9844e7dbc25"
      },
      "source": [
        "# Fit the model to the training data\n",
        "results = model.fit(x_train, y_train, epochs=3, batch_size=64,validation_data=(x_test, y_test))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "391/391 [==============================] - 19s 39ms/step - loss: 0.5557 - accuracy: 0.7192 - val_loss: 0.3775 - val_accuracy: 0.8365\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 14s 37ms/step - loss: 0.2832 - accuracy: 0.8899 - val_loss: 0.3586 - val_accuracy: 0.8409\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 15s 37ms/step - loss: 0.1945 - accuracy: 0.9322 - val_loss: 0.4031 - val_accuracy: 0.8367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CrZgxje8w2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26aef334-111e-4385-fe66-56b7de5e4962"
      },
      "source": [
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=64)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 3s 8ms/step - loss: 0.4031 - accuracy: 0.8367\n",
            "Test score: 0.40308964252471924\n",
            "Test accuracy: 0.8367199897766113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCE3OEZNzQzA"
      },
      "source": [
        "**Using Simple RNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qklhcSWay9f-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5209de58-0243-4932-8002-c385b78e9617"
      },
      "source": [
        "# Define the layers in the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_vector_length, input_length=max_review_length))\n",
        "#model.add(LSTM(128))\n",
        "model.add(SimpleRNN(128,dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(model.summary())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 100, 128)          1280000   \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 1,313,025\n",
            "Trainable params: 1,313,025\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB-Msn79zZgi"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "             optimizer='adam', \n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfYP4xEuzeK7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59b4670e-0f90-4367-e319-b6b880d51c80"
      },
      "source": [
        "# Fit the model to the training data\n",
        "results = model.fit(x_train, y_train, epochs=3, batch_size=64, validation_data=(x_test, y_test))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "391/391 [==============================] - 62s 151ms/step - loss: 0.7025 - accuracy: 0.5154 - val_loss: 0.6812 - val_accuracy: 0.5499\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 59s 150ms/step - loss: 0.6424 - accuracy: 0.6091 - val_loss: 0.7332 - val_accuracy: 0.6783\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 59s 151ms/step - loss: 0.4509 - accuracy: 0.7948 - val_loss: 0.5057 - val_accuracy: 0.7756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVKPgiTfzfgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e6ce08-0da4-4909-987b-85478fc1f34f"
      },
      "source": [
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=64)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 4s 9ms/step - loss: 0.5057 - accuracy: 0.7756\n",
            "Test score: 0.505683958530426\n",
            "Test accuracy: 0.7756400108337402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec0zq23Df-Kz"
      },
      "source": [
        ""
      ]
    }
  ]
}